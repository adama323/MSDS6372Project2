#like SAS does.  Using the summary coefficients we can generate CI for each one in the table
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
#This is due to the fact that cylinders are correlated with everything.  Go back to EDA and verify.  We just don't
#see the VIF's look too suspect.
t(aggregate(horsepower~cylinders,data=newAuto,summary))
plot(horsepower~cylinders,data=newAuto)
#For this scenario it might be helpful to just manually fit some models first.
#If one were to conduct forward selection (see below (NO cv/ test set, just AIC selected)), R would want to keep all of the
#highly correlated predictors in questions and the same interpretation problem occurs.
model.null<-glm(mpg ~ 1, data=newAuto,family = binomial(link="logit"))
#This starts with a null model and then builds up using forward selection up to all the predictors that were specified in my
#main model previously.
step(model.null,
scope = list(upper=model.main),
direction="forward",
test="Chisq",
data=newAuto)
#To deal with the redundamcy, I would throw the cylinder variable out and then see what happens
model.main<-glm(mpg ~ displacement+horsepower+weight+acceleration, data=newAuto,family = binomial(link="logit"))
summary(model.main)
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
vif(model.main)
#Residual diagnostics can be obtained using
plot(model.main)
#With a simplistic model with no lack of fit issues, we can beging providing statistical inference if no
#interactions are present
summary(model.main)
#I'm not aware of a nice little automated way to produce Odds ratio metrics
#like SAS does.  Using the summary coefficients we can generate CI for each one in the table
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
#Playing around with adding more complexity to see if anything sticks compared to the simpler model
model.complex<-glm(mpg ~ displacement+horsepower+weight+acceleration+horsepower:weight+displacement:horsepower+weight:acceleration, data=newAuto,family = binomial(link="logit"))
step(model.main,
scope = list(upper=model.complex),
direction="forward",
test="Chisq",
data=newAuto)
hoslem.test(model.complex$y, fitted(model.complex), g=10)
summary(glm(mpg ~ displacement+horsepower+weight+acceleration+displacement:horsepower+weight:acceleration, data=newAuto,family = binomial(link="logit")))
library(glmnet)
library(bestglm)
dat.train.x <- model.matrix(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin-1,newAuto)
dat.train.y<-newAuto[,1]
library(glmnet)
cvfit <- cv.glmnet(dat.train.x, dat.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is little below .1
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]
#Optimal penalty
cvfit$lambda.min
#For final model predictions go ahead and refit lasso using entire
#data set
finalmodel<-glmnet(dat.train.x, dat.train.y, family = "binomial",lambda=cvfit$lambda.min)
#Get training set predictions...We know they are biased but lets create ROC's.
#These are predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit.pred <- predict(finalmodel, newx = dat.train.x, type = "response")
#Create ROC curves (Remember if you have a test data set, you can use that to compare models)
library(ROCR)
pred <- prediction(fit.pred[,1], dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
#Plot ROC
plot(roc.perf,main="LASSO")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#In addition to LASSO, if we are concerned that the biased estiamtes
#are affecting our model, we can go back and refit using regular
#regression removing the variables that have no importance.
coef(finalmodel)
olog<-glm(mpg~cylinders+horsepower+weight+year+origin,data=newAuto,family=binomial)
fit.pred <- predict(olog, newx = dat.train.x, type = "response")
pred <- prediction(fit.pred, dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
#Plot ROC
plot(roc.perf,main="Ordingary Logistic")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
dat.train.x
summary(olog)
dat.train.y
library(glmnet)
library(ROCR)
library(MASS)
library(ggplot2)
library(pheatmap)
library(randomForest)
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/Unit13")
dat <- read.csv("CancerExample.csv", header = TRUE)
#Get Training Set
dat.train <- dat[which(dat$Set == "Training"),]
dat.train.x <- dat.train[,6:ncol(dat.train)]
dat.train.y <- dat.train$Censor
dat.train.y <- as.factor(as.character(dat.train.y))
pc.result<-prcomp(dat.train.x,scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$Censor<-dat.train.y
#Loadings for interpretation
pc.result$rotation
#Scree plot
pc.eigen<-(pc.result$sdev)^2
pc.prop<-pc.eigen/sum(pc.eigen)
pc.cumprop<-cumsum(pc.prop)
plot(1:9,pc.prop,type="l",main="Scree Plot",ylim=c(0,1),xlab="PC #",ylab="Proportion of Variation")
lines(1:9,pc.cumprop,lty=3)
#Use ggplot2 to plot the first few pc's
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
geom_point(aes(col=Censor), size=1)+
geom_hline(yintercept = 0, colour = "gray65") +
geom_vline(xintercept = 0, colour = "gray65") +
ggtitle("PCA plot of Gene Expression Data")
ggplot(data = pc.scores, aes(x = PC1, y = PC3)) +
geom_point(aes(col=Censor), size=1)+
geom_hline(yintercept = 0, colour = "gray65") +
geom_vline(xintercept = 0, colour = "gray65") +
ggtitle("PCA plot of Gene Expression Data")
ggplot(data = pc.scores, aes(x = PC2, y = PC3)) +
geom_point(aes(col=Censor), size=1)+
geom_hline(yintercept = 0, colour = "gray65") +
geom_vline(xintercept = 0, colour = "gray65") +
ggtitle("PCA plot of Gene Expression Data")
library(RColorBrewer)
cols <- colorRampPalette(brewer.pal(9, "Set1"))
x<-t(dat.train.x)
colnames(x)<-dat.train.y
pheatmap(x,annotation_col=data.frame(Cancer=dat.train.y),annotation_colors=list(Cancer=c("0"="white","1"="green")),scale="row",legend=T,color=colorRampPalette(c("blue","white", "red"), space = "rgb")(100))
#glmnet requires a matrix
dat.train.x <- as.matrix(dat.train.x)
library(glmnet)
cvfit <- cv.glmnet(dat.train.x, dat.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#Get training set predictions...We know they are biased but lets create ROC's.
#These are predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit.pred <- predict(cvfit, newx = dat.train.x, type = "response")
#Compare the prediction to the real outcome
head(fit.pred)
head(dat.train.y)
#Create ROC curves
pred <- prediction(fit.pred[,1], dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
#Plot ROC
plot(roc.perf)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Get Validation Set I
dat.val1 <- dat[which(dat$Set == "Validation I"),]
dat.val1.x <- dat.val1[,c(6:ncol(dat))]
dat.val1.x <- as.matrix(dat.val1.x)
dat.val1.y <- dat.val1$Censor
dat.val1.y <- as.factor(as.character(dat.val1.y))
#Run model from training set on valid set I
fit.pred1 <- predict(cvfit, newx = dat.val1.x, type = "response")
#ROC curves
pred1 <- prediction(fit.pred1[,1], dat.val1.y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.val1 <- performance(pred1, measure = "auc")
auc.val1 <- auc.val1@y.values
plot(roc.perf1)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.val1[[1]],3), sep = ""))
#Rinse and Repeat for valid set II and III
dat.val2 <- dat[which(dat$Set == "Validation II"),]
dat.val2.x <- dat.val2[,c(6:ncol(dat))]
dat.val2.x <- as.matrix(dat.val2.x)
dat.val2.y <- dat.val2$Censor
dat.val2.y <- as.factor(as.character(dat.val2.y))
fit.pred2 <- predict(cvfit, newx = dat.val2.x, type = "response")
pred2 <- prediction(fit.pred2[,1], dat.val2.y)
roc.perf2 = performance(pred2, measure = "tpr", x.measure = "fpr")
auc.val2 <- performance(pred2, measure = "auc")
auc.val2 <- auc.val2@y.values
plot(roc.perf2)
abline(a=0, b= 1)
text(x = .42, y = .6,paste("AUC = ", round(auc.val2[[1]],3), sep = ""))
#Valid set III
dat.val3 <- dat[which(dat$Set == "Validation III"),]
dat.val3.x <- dat.val3[,c(6:ncol(dat))]
dat.val3.x <- as.matrix(dat.val3.x)
dat.val3.y <- dat.val3$Censor
dat.val3.y <- as.factor(as.character(dat.val3.y))
fit.pred3 <- predict(cvfit, newx = dat.val3.x, type = "response")
pred3 <- prediction(fit.pred3[,1], dat.val3.y)
roc.perf3 = performance(pred3, measure = "tpr", x.measure = "fpr")
auc.val3 <- performance(pred3, measure = "auc")
auc.val3 <- auc.val3@y.values
plot(roc.perf3)
abline(a=0, b= 1)
text(x = .4, y = .6,paste("AUC = ", round(auc.val3[[1]],3), sep = ""))
#This is helpful:  https://www.r-bloggers.com/a-small-introduction-to-the-rocr-package/ that does some extra things you might find
#helpful.
#If you want to mess around with other packages: https://rviews.rstudio.com/2019/03/01/some-r-packages-for-roc-curves/
plot( roc.perf1, colorize = TRUE)
plot(roc.perf2, add = TRUE, colorize = TRUE)
plot(roc.perf3, add = TRUE, colorize = TRUE)
abline(a=0, b= 1)
#without color for cutoff; but adding colors to allow for comarisons of the curves
plot( roc.perf1)
plot(roc.perf2,col="orange", add = TRUE)
plot(roc.perf3,col="blue", add = TRUE)
legend("bottomright",legend=c("Valid 1","Valid 2","Valid 3"),col=c("black","orange","blue"),lty=1,lwd=1)
abline(a=0, b= 1)
#Training Set
dat.train <- dat[which(dat$Set == "Training"),]
dat.train.x <- dat.train[,6:ncol(dat)]
dat.train.y <- dat.train$Censor
dat.train.y <- as.factor(as.character(dat.train.y))
fit.lda <- lda(dat.train.y ~ ., data = dat.train.x)
pred.lda <- predict(fit.lda, newdata = dat.train.x)
preds <- pred.lda$posterior
preds <- as.data.frame(preds)
pred <- prediction(preds[,2],dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Valid set I
dat.val1 <- dat[which(dat$Set == "Validation I"),]
dat.val1.x <- dat.val1[,c(6:ncol(dat))]
dat.val1.y <- dat.val1$Censor
dat.val1.y <- as.factor(as.character(dat.val1.y))
pred.lda1 <- predict(fit.lda, newdata = dat.val1.x)
preds1 <- pred.lda1$posterior
preds1 <- as.data.frame(preds1)
pred1 <- prediction(preds1[,2],dat.val1.y)
roc.perf = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred1, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Valid set II
dat.val2 <- dat[which(dat$Set == "Validation II"),]
dat.val2.x <- dat.val2[,c(5:ncol(dat))]
dat.val2.y <- dat.val2$Censor
dat.val2.y <- as.factor(as.character(dat.val2.y))
pred.lda2 <- predict(fit.lda, newdata = dat.val2.x)
preds2 <- pred.lda2$posterior
preds2 <- as.data.frame(preds2)
pred2 <- prediction(preds2[,2],dat.val2.y)
roc.perf = performance(pred2, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred2, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Valid set III
dat.val3 <- dat[which(dat$Set == "Validation III"),]
dat.val3.x <- dat.val3[,c(5:ncol(dat))]
dat.val3.y <- dat.val3$Censor
dat.val3.y <- as.factor(as.character(dat.val3.y))
pred.lda3 <- predict(fit.lda, newdata = dat.val3.x)
preds3 <- pred.lda3$posterior
preds3 <- as.data.frame(preds3)
pred3 <- prediction(preds3[,2],dat.val3.y)
roc.perf = performance(pred3, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred3, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#One more time with a Random Forest
#I'm removing the fluff variables just to make coding easier.
dat.train.rf <- dat[which(dat$Set == "Training"),-(1:4)]
dat.train.rf$Censor<-factor(dat.train.rf$Censor)
train.rf<-randomForest(Censor~.,data=dat.train.rf,mtry=4,ntree=500,importance=T)
fit.pred<-predict(train.rf,newdata=dat.train.rf,type="prob")
pred <- prediction(fit.pred[,2], dat.train.rf$Censor)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Predict Validation Set I
dat.val1.rf <- dat.val1[,-(1:4)]
pred.val1<-predict(train.rf,newdata=dat.val1.rf,type="prob")
pred <- prediction(pred.val1[,2], dat.val1.rf$Censor)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Predict Validation Set 2
dat.val2.rf <- dat.val2[,-(1:4)]
pred.val2<-predict(train.rf,newdata=dat.val2.rf,type="prob")
pred <- prediction(pred.val2[,2], dat.val2.rf$Censor)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#Predict Validation Set 3
dat.val3.rf <- dat.val3[,-(1:4)]
pred.val3<-predict(train.rf,newdata=dat.val3.rf,type="prob")
pred <- prediction(pred.val3[,2], dat.val3.rf$Censor)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
nloops<-50   #number of CV loops
ntrains<-dim(dat.train.x)[1]  #No. of samples in training data set
cv.aucs<-c() #initializing a vector to store the auc results for each CV run
for (i in 1:nloops){
index<-sample(1:ntrains,60)
cvtrain.x<-as.matrix(dat.train.x[index,])
cvtest.x<-as.matrix(dat.train.x[-index,])
cvtrain.y<-dat.train.y[index]
cvtest.y<-dat.train.y[-index]
cvfit <- cv.glmnet(cvtrain.x, cvtrain.y, family = "binomial", type.measure = "class")
fit.pred <- predict(cvfit, newx = cvtest.x, type = "response")
pred <- prediction(fit.pred[,1], cvtest.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
cv.aucs[i]<-auc.train[[1]]
}
hist(cv.aucs)
summary(cv.aucs)
newAuto<-Auto
#creating binary response for illustration
newAuto$mpg<-factor(ifelse(Auto$mpg>median(Auto$mpg),"High","Low"))
#DATA entry and EDA.  You guys are familiar with this data set already.  Its the same as the PCA Unit 9 code.
library(ISLR)
dim(Auto)
newAuto<-Auto
#creating binary response for illustration
newAuto$mpg<-factor(ifelse(Auto$mpg>median(Auto$mpg),"High","Low"))
newAuto$cylinders<-factor(newAuto$cylinders)
newAuto$origin<-factor(newAuto$origin)
newAuto$mpg<-relevel(newAuto$mpg, ref = "Low")  #""Yes/1" is high mpg "No/0" is low mpg.  This forces the odds to be what you want it to be.
library(caret)
confusionMatrix(newAuto$mpg,newAuto$mpg)
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project2/Adult-Income")
library(tidyverse)
adult = read.csv("adult.data.csv",header=FALSE,sep=",",strip.white=TRUE)
# Assign column names
colnames(adult) <- c(
"age", # continuous
"workclass",# Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked
"fnlwgt", # continous, final weight. In other words, this is the number of people the census believes the entry represents
"education", # Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th,
# Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.education-num: continuous
"education.num", # continuous
"marital.status", # Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
"occupation",  # Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct,                 # Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces
"relationship", # Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried
"race", # White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black
"sex",   # Female, Male
"capital.gain", # continuous
"capital.loss", # continuous
"hours.per.week", # continuous
"native.country", # United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands
"income" # expects to predict
)
str(adult)
## First, convert "?"s to Not-Known...
adult[adult == "?"] <- "Not-Known"
# Decision is made to delete Not-Known observations under workclass & occupation (based on EDA below)
# 1. workclass and occupation missing values are almost identical - except a few workclass assigned Never-worked has a few Not-Known
# workclass and occupation seem to both being good potential predictors for response income
# Thus decision is made to delete Not-Known under occupation and workclass - restrict our range to only predict observation that has occupation or workclass that is known to us
adult = adult%>% filter(workclass!="Not-Known" & occupation!="Not-Known")
# Reorder education levels for later
edu.levels = factor(c("Preschool","1st-4th","5th-6th","7th-8th","9th","10th","11th","12th","HS-grad","Some-college","Assoc-voc","Assoc-acdm","Bachelors","Masters","Prof-school","Doctorate"))
adult$education <- factor(adult$education, levels = edu.levels)
variables.to.factor = c("workclass","marital.status","occupation","relationship","race","sex","native.country","income")
adult[variables.to.factor] = lapply(adult[variables.to.factor],factor)
# Create variable cp.eover7298 per EDA discovery - capital gain equal or over 7298 - using "No" as reference
adult$cp.eover7298<-factor(ifelse(adult$capital.gain>=7298,"Yes","No"),levels=c("No","Yes"))
summary(adult)
# inporting testing data
adult.test = read.csv("adult.test.csv",header=FALSE,sep=",",strip.white=TRUE)
colnames(adult.test) <- c(
"age", # continuous
"workclass",# Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked
"fnlwgt", # continous, final weight. In other words, this is the number of people the census believes the entry represents
"education", # Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th,
# Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.education-num: continuous
"education.num", # continuous
"marital.status", # Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
"occupation",  # Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct,                 # Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces
"relationship", # Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried
"race", # White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black
"sex",   # Female, Male
"capital.gain", # continuous
"capital.loss", # continuous
"hours.per.week", # continuous
"native.country", # United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands
"income" # expects to predict
)
## First, convert "?"s to Not-Known...
adult.test[adult.test == "?"] <- "Not-Known"
# Similar decision is made to delete workclass that has Not-Known to utilize both workclass and occupation
adult.test = adult.test %>% filter(workclass!="Not-Known"& occupation!="Not-Known")
adult.test$education <- factor(adult.test$education, levels=edu.levels)
variables.to.factor = c("workclass","marital.status","occupation","relationship","race","sex","native.country","income")
adult.test[variables.to.factor] = lapply(adult.test[variables.to.factor],factor)
# Create variable cp.eover7298 per EDA discovery - capital gain equal or over 7298 - using "No" as reference
adult.test$cp.eover7298<-factor(ifelse(adult.test$capital.gain>=7298,"Yes","No"),levels=c("No","Yes"))
summary(adult.test)
#adult.missing = adult %>% filter_all(any_vars(is.na(.)))
#adult.test.missing = adult.test %>% filter_all(any_vars(is.na(.)))
# Look at missing data
library(naniar)
gg_miss_var(adult)
gg_miss_var(adult.test)
# construct the LDA model
# Fron EDA we know fnlwgt is not helpful in identifying income - will not use it here
# We will use education.num since it is a substitute for education
mylda <- lda(income ~ age + education.num+ capital.gain+capital.loss+hours.per.week, data = adult)
mylda.prd <- predict(mylda, newdata = adult.test)
mylda.posterior <- mylda.prd$posterior[,2]
# Generate ROC curve to find a cutoff to use
results.lda <-prediction(mylda.posterior, adult.test$income,label.ordering=c("<=50K.",">50K."))
roc.lda = performance(results.lda, measure = "tpr", x.measure = "fpr")
plot(roc.lda,colorize = TRUE)
abline(a=0, b= 1)
auc.lda <- performance(results.lda, measure = "auc")
auc.lda <- auc.lda@y.values
text(x = .40, y = .6,paste("AUC = ", round(auc.lda[[1]],3), sep = ""))
#Calculate logistic regression complex using cut of 0.25 on test set
cutoff1<-0.23
class.lda <-factor(ifelse(mylda.posterior>cutoff1,">50K.","<=50K."),levels=c("<=50K.",">50K."))
confusionMatrix(class.lda,adult.test$income)
##################################################################################################
# construct the QDA model
# Fron EDA we know fnlwgt is not helpful in identifying income - will not use it here
# We will use education.num since it is a substitute for education
myqda <- qda(income ~ age + education.num+ capital.gain+capital.loss+hours.per.week, data = adult)
myqda.prd <- predict(myqda, newdata = adult.test)
myqda.posterior <- myqda.prd$posterior[,2]
# Generate ROC curve to find a cutoff to use
results.qda <-prediction(myqda.posterior, adult.test$income,label.ordering=c("<=50K.",">50K."))
roc.qda = performance(results.qda, measure = "tpr", x.measure = "fpr")
plot(roc.qda,colorize = TRUE)
abline(a=0, b= 1)
auc.qda <- performance(results.qda, measure = "auc")
auc.qda <- auc.qda@y.values
text(x = .40, y = .6,paste("AUC = ", round(auc.qda[[1]],3), sep = ""))
results.lda <-prediction(mylda.posterior, adult.test$income,label.ordering=c("<=50K.",">50K."))
roc.lda = performance(results.lda, measure = "tpr", x.measure = "fpr")
plot(roc.lda,colorize = TRUE)
abline(a=0, b= 1)
auc.lda <- performance(results.lda, measure = "auc")
auc.lda <- auc.lda@y.values
text(x = .40, y = .6,paste("AUC = ", round(auc.lda[[1]],3), sep = ""))
# As we can see the lda model is slighly less good compared to logistic regression model
# The ROC curve has shifted downwards compared to LR - AUC =0.809(decreased)
# Nevertheless to balance off performance between high True positive and low False positive we can use around 0.25
#Calculate logistic regression complex using cut of 0.25 on test set
cutoff1<-0.23
class.lda <-factor(ifelse(mylda.posterior>cutoff1,">50K.","<=50K."),levels=c("<=50K.",">50K."))
confusionMatrix(class.lda,adult.test$income)
results.qda <-prediction(myqda.posterior, adult.test$income,label.ordering=c("<=50K.",">50K."))
roc.qda = performance(results.qda, measure = "tpr", x.measure = "fpr")
plot(roc.qda,colorize = TRUE)
abline(a=0, b= 1)
auc.qda <- performance(results.qda, measure = "auc")
auc.qda <- auc.qda@y.values
text(x = .40, y = .6,paste("AUC = ", round(auc.qda[[1]],3), sep = ""))
# AUC seem to be slightly better than LDA AUC = 0.823
# To balance off performance between high True positive and low False positive we can use around 0.25
#Calculate logistic regression complex using cut of 0.015 on test set
cutoff2<-0.015
class.qda <-factor(ifelse(myqda.posterior>cutoff2,">50K.","<=50K."),levels=c("<=50K.",">50K."))
confusionMatrix(class.qda,adult.test$income)
hist(myqda.posterior)
hist(mylda.posterior)
library(GGally)
#create temp variable in order to fit lm model to test vif
temp = as.numeric(adult$income)
linear.model = lm(temp~.-income-education.num-occupation,data=adult)
library(car) # where vif lives
summary(linear.model)
vif(linear.model)
# remove relationship variable see if GVIF gets better
linear.model1 = lm(temp~.-income-education.num-occupation-relationship,data=adult)
vif(linear.model1)
table(marital.status,relationship)
# Which variable is more associated with the response though? - try chi-squared test approximation(since dataset is quite large)
attach(adult)
chi.test <- chisq.test(table(income,marital.status))
chi.test
# It will be interesting to first perform a PCA and look at how numeric variables can contribute to predicting the response
reduced<-adult[,-c(2,4,6,7,8,9,10,14,15,16)]
# Do some exploration between continuous variable and categorical predictors
plot(age~education) # Not particularly interesting
plot(hours.per.week~education) # Not particularly interesting
plot(capital.gain~cp.eover7298) # variable created
plot(age~marital.status) # Widowed have the highest mean age
# Explore some continuous variables predicting response
plot(age~income,col=c("red","blue")) # Might be a good predictor - mean are slighly different
plot(fnlwgt~income,col=c("red","blue")) # Almost identical between two groups - may not be a good predictor
plot(education.num~income,col=c("red","blue")) # education.num derived from education may be a good predictor if only continuous variable can be used
plot(capital.gain~income,col=c("red","blue")) # There is a floor value of 0 for both groups
summary(capital.gain)
plot(capital.loss~income,col=c("red","blue")) #similar to capital gain most values are around 0
plot(have.capital.loss$capital.loss~have.capital.loss$income,col=c("red","blue")) # mean seem to significantly different from each other
plot(hours.per.week~income,col=c("red","blue")) # hours.per.week can potentially be a good predictor since there seem to have a good seperation between mean of the two groups
