newAuto$mpg<-relevel(newAuto$mpg, ref = "Low")  #""Yes/1" is high mpg "No/0" is low mpg.  This forces the odds to be what you want it to be.
#
#aggregate is good for summary stats by groups for continous predictors
aggregate(weight~mpg,data=newAuto,summary)
aggregate(displacement~mpg,data=newAuto,summary)
#lets attach newAuto so we don't have to keep writing newAuto$
attach(newAuto)
#Table of counts like proc freq are helpful for categorcal predictors
ftable(addmargins(table(mpg,cylinders)))
#It probably is wise to throw out the 3 and 5 cylinder ones or combine it with
#four or six.  I'll remove to keep it short.
newAuto<-newAuto[-which(cylinders %in% c(3,5)),]
attach(newAuto)
cylinders=factor(cylinders)
levels(cylinders)
ftable(addmargins(table(mpg,origin)))
ftable(addmargins(table(mpg,year)))
#to get proportions that make sense
prop.table(table(mpg,cylinders),2)
prop.table(table(mpg,origin),2)
prop.table(table(mpg,year),2)
#Visualize
plot(mpg~cylinders,col=c("red","blue"))
plot(mpg~origin,col=c("red","blue"))
plot(mpg~year,col=c("red","blue"))
#Visualize
plot(weight~mpg,col=c("red","blue"))
plot(acceleration~mpg,col=c("red","blue"))
plot(displacement~mpg,col=c("red","blue"))
#Examine the correlation between the continous predictors
pairs(newAuto[,3:6])
my.cor<-cor(newAuto[,3:6])
my.cor
pairs(newAuto[,3:6],col=mpg)
#If you have a lot of predictors, heatmap with correlations could
#be helpful to examine redundancy.
library(gplots)
library(ggplot2)
heatmap.2(my.cor,col=redgreen(75),
density.info="none", trace="none", dendrogram=c("row"),
symm=F,symkey=T,symbreaks=T, scale="none")
#Another option here would be to do PCA among the continous predictors to see
#if they seperate out.  Or a heatmap.
pc.result<-prcomp(newAuto[,3:6],scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$mpg<-newAuto$mpg
#Use ggplot2 to plot the first few pc's
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
geom_point(aes(col=mpg), size=1)+
ggtitle("PCA of Auto")
##############################
#Interpretation
#The purpose of this code is to illustrate some basic functionality of logistic regression in R.
#Some of the continuous variables look redundant, but for start we will just include everything.
newAuto<-na.omit(newAuto)
model.main<-glm(mpg ~ cylinders+displacement+horsepower+weight+acceleration+year, data=newAuto,family = binomial(link="logit"))
library(ResourceSelection)
library(car)
#Using this tool, GVIF is the same as VIF for continuous predictors only
#For categorical predictors, the value GVIG^(1/(2*df)) should be squared and interpreted
#as a usuaul vif type metric.The following code can be used to interpret VIFs like we
#discussed in class.
(vif(model.main)[,3])^2
vif(model.main)
#As expected displacement and the other continuous variables have moderately high VIF.  Based on the pairwise scatterplots
#I believe it is clear.  It is also apparent that cylinders is associated with some of the predictors
#as well. Higher cylinders tends to produce higher horsepower.
#Remember we should not only resort to things like VIF, we should look at the output and
#see if things make sense.
attach(newAuto)
prop.table(table(mpg,cylinders),2)
t(aggregate(weight~cylinders,data=newAuto,summary))
t(aggregate(acceleration~cylinders,data=newAuto,summary))
t(aggregate(horsepower~cylinders,data=newAuto,summary))
t(aggregate(displacement~cylinders,data=newAuto,summary))
#Hosmer Lemeshow test for lack of fit.  Use as needed.  The g=10 is an option that deals with the continuous predictors if any are there.
#This should be increased with caution.
hoslem.test(model.main$y, fitted(model.main), g=10)
#Summary of current fit
summary(model.main)
#I'm not aware of a nice little automated way to produce Odds ratio metrics
#like SAS does.  Using the summary coefficients we can generate CI for each one in the table
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
#This is due to the fact that cylinders are correlated with everything.  Go back to EDA and verify.  We just don't
#see the VIF's look too suspect.
t(aggregate(horsepower~cylinders,data=newAuto,summary))
plot(horsepower~cylinders,data=newAuto)
#For this scenario it might be helpful to just manually fit some models first.
#If one were to conduct forward selection (see below (NO cv/ test set, just AIC selected)), R would want to keep all of the
#highly correlated predictors in questions and the same interpretation problem occurs.
model.null<-glm(mpg ~ 1, data=newAuto,family = binomial(link="logit"))
#This starts with a null model and then builds up using forward selection up to all the predictors that were specified in my
#main model previously.
step(model.null,
scope = list(upper=model.main),
direction="forward",
test="Chisq",
data=newAuto)
#To deal with the redundamcy, I would throw the cylinder variable out and then see what happens
model.main<-glm(mpg ~ displacement+horsepower+weight+acceleration, data=newAuto,family = binomial(link="logit"))
summary(model.main)
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
vif(model.main)
#Residual diagnostics can be obtained using
plot(model.main)
#With a simplistic model with no lack of fit issues, we can beging providing statistical inference if no
#interactions are present
summary(model.main)
#I'm not aware of a nice little automated way to produce Odds ratio metrics
#like SAS does.  Using the summary coefficients we can generate CI for each one in the table
exp(cbind("Odds ratio" = coef(model.main), confint.default(model.main, level = 0.95)))
#Playing around with adding more complexity to see if anything sticks compared to the simpler model
model.complex<-glm(mpg ~ displacement+horsepower+weight+acceleration+horsepower:weight+displacement:horsepower+weight:acceleration, data=newAuto,family = binomial(link="logit"))
step(model.main,
scope = list(upper=model.complex),
direction="forward",
test="Chisq",
data=newAuto)
hoslem.test(model.complex$y, fitted(model.complex), g=10)
summary(glm(mpg ~ displacement+horsepower+weight+acceleration+displacement:horsepower+weight:acceleration, data=newAuto,family = binomial(link="logit")))
library(glmnet)
library(bestglm)
dat.train.x <- model.matrix(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin-1,newAuto)
dat.train.y<-newAuto[,1]
library(glmnet)
cvfit <- cv.glmnet(dat.train.x, dat.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate is little below .1
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]
#Optimal penalty
cvfit$lambda.min
#For final model predictions go ahead and refit lasso using entire
#data set
finalmodel<-glmnet(dat.train.x, dat.train.y, family = "binomial",lambda=cvfit$lambda.min)
#Get training set predictions...We know they are biased but lets create ROC's.
#These are predicted probabilities from logistic model  exp(b)/(1+exp(b))
fit.pred <- predict(finalmodel, newx = dat.train.x, type = "response")
#Create ROC curves (Remember if you have a test data set, you can use that to compare models)
library(ROCR)
pred <- prediction(fit.pred[,1], dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
#Plot ROC
plot(roc.perf,main="LASSO")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
#In addition to LASSO, if we are concerned that the biased estiamtes
#are affecting our model, we can go back and refit using regular
#regression removing the variables that have no importance.
coef(finalmodel)
olog<-glm(mpg~cylinders+horsepower+weight+year+origin,data=newAuto,family=binomial)
fit.pred <- predict(olog, newx = dat.train.x, type = "response")
pred <- prediction(fit.pred, dat.train.y)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred, measure = "auc")
auc.train <- auc.train@y.values
#Plot ROC
plot(roc.perf,main="Ordingary Logistic")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))
dat.train.x
summary(olog)
dat.train.y
library(ISLR)
attach(Carseats)
High = ifelse(Sales<=8,"No","Yes")
Carseats = data.frame(Carseats,High)
# Use tree to fit a classification tree in order to predict High using all variables but Sales
tree.carseats = tree(High~.-Sales,Carseats)
library(tree)
install.packages("tree")
library(tree)
# Use tree to fit a classification tree in order to predict High using all variables but Sales
tree.carseats = tree(High~.-Sales,Carseats)
summary(tree.carseats)
# Use tree to fit a classification tree in order to predict High using all variables but Sales
tree.carseats = tree(High~.-Sales,Carseats)
summary(tree.carseats)
str(Carseats)
summary(Carseats)
view(Carseats)
library(tidyverse)
view(Carseats)
str(Carseats)
Carseats$High = as.factor(Carseats$High)
# Use tree to fit a classification tree in order to predict High using all variables but Sales
tree.carseats = tree(High~.-Sales,Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
tree.carseats
# split dataset
set.seed(2)
train=sample(1:nrow(Carseats),200)
Carseats.test = Carseats[-train,]
High.test = High[-train]
tree.carseats = tree(High~.-Sales,Carseats,subset=train)
tree.pred = predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(104+50)/200
dim(Carseats)
# Next, we consider whether pruning the tree might lead to improved results
set.seed(3)
cv.carseats = cv.tree(tree.carseats, FUN=prune.misclass)
names(cv.carseats)
cv.carseats
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
prune.carseats = prune.misclass(tree.carseats,best=8)
plot(prune.carseats)
test(prune.carseats,pretty=0)
text(prune.carseats,pretty=0)
# How well is this?
tree.pred = predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(89+62)/200
(104+50)/200
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston),nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type="b")
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
text(prune.boston,pretty=0)
yhat = predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag=predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
mean((yhat.bag-boston.test)^2)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf=predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
importance(rf.boston)
varImpPlot(rf.boston)
library(gbm)
install.packages("gbm")
library(gbm)
dim(train)
dim(Boston)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
set.seed(1)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.boston)
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
yhat.boost = predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
# Changing the shrinkage parameter
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=-.2,verbose=F)
yhat.boost = predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
head(yhat.boost)
# Changing the shrinkage parameter
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost = predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project2/Adult-Income")
library(tidyverse)
adult = read.csv("adult.data.csv",header=FALSE,sep=",",strip.white=TRUE)
# Assign column names
colnames(adult) <- c(
"age", # continuous
"workclass",# Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked
"fnlwgt", # continous, final weight. In other words, this is the number of people the census believes the entry represents
"education", # Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th,
# Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.education-num: continuous
"education.num", # continuous
"marital.status", # Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
"occupation",  # Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct,                 # Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces
"relationship", # Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried
"race", # White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black
"sex",   # Female, Male
"capital.gain", # continuous
"capital.loss", # continuous
"hours.per.week", # continuous
"native.country", # United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands
"income" # expects to predict
)
str(adult)
## First, convert "?"s to Not-Known...
adult[adult == "?"] <- "Not-Known"
# Decision is made to delete Not-Known observations under workclass & occupation (based on EDA below)
# 1. workclass and occupation missing values are almost identical - except a few workclass assigned Never-worked has a few Not-Known
# workclass and occupation seem to both being good potential predictors for response income
# Thus decision is made to delete Not-Known under occupation and workclass - restrict our range to only predict observation that has occupation or workclass that is known to us
adult = adult%>% filter(workclass!="Not-Known" & occupation!="Not-Known")
# Reorder education levels for later
edu.levels = factor(c("Preschool","1st-4th","5th-6th","7th-8th","9th","10th","11th","12th","HS-grad","Some-college","Assoc-voc","Assoc-acdm","Bachelors","Masters","Prof-school","Doctorate"))
adult$education <- factor(adult$education, levels = edu.levels)
variables.to.factor = c("workclass","marital.status","occupation","relationship","race","sex","native.country","income")
adult[variables.to.factor] = lapply(adult[variables.to.factor],factor)
# Create variable cp.eover7298 per EDA discovery - capital gain equal or over 7298 - using "No" as reference
adult$cp.eover7298<-factor(ifelse(adult$capital.gain>=7298,"Yes","No"),levels=c("No","Yes"))
summary(adult)
# inporting testing data
adult.test = read.csv("adult.test.csv",header=FALSE,sep=",",strip.white=TRUE)
colnames(adult.test) <- c(
"age", # continuous
"workclass",# Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked
"fnlwgt", # continous, final weight. In other words, this is the number of people the census believes the entry represents
"education", # Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th,
# Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.education-num: continuous
"education.num", # continuous
"marital.status", # Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
"occupation",  # Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct,                 # Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces
"relationship", # Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried
"race", # White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black
"sex",   # Female, Male
"capital.gain", # continuous
"capital.loss", # continuous
"hours.per.week", # continuous
"native.country", # United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands
"income" # expects to predict
)
## First, convert "?"s to Not-Known...
adult.test[adult.test == "?"] <- "Not-Known"
# Similar decision is made to delete workclass that has Not-Known to utilize both workclass and occupation
adult.test = adult.test %>% filter(workclass!="Not-Known"& occupation!="Not-Known")
adult.test$education <- factor(adult.test$education, levels=edu.levels)
variables.to.factor = c("workclass","marital.status","occupation","relationship","race","sex","native.country","income")
adult.test[variables.to.factor] = lapply(adult.test[variables.to.factor],factor)
# Create variable cp.eover7298 per EDA discovery - capital gain equal or over 7298 - using "No" as reference
adult.test$cp.eover7298<-factor(ifelse(adult.test$capital.gain>=7298,"Yes","No"),levels=c("No","Yes"))
summary(adult.test)
#adult.missing = adult %>% filter_all(any_vars(is.na(.)))
#adult.test.missing = adult.test %>% filter_all(any_vars(is.na(.)))
# Look at missing data
library(naniar)
gg_miss_var(adult)
gg_miss_var(adult.test)
library(MASS)
library(glmnet)
# Generate ROC curve to find a cutoff to use
library(ROCR)
library(caret)
library(randomForest)
set.seed(1)
# By default use sqrt(p) number variables when building a random forest of classification trees
rf.adult = randomForest(income~.,data=adult)
levels(adult.test$education)<-levels(adult$education)
levels(adult.test$marital.status)<-levels(adult$marital.status)
levels(adult.test$occupation)<-levels(adult$occupation)
levels(adult.test$relationship)<-levels(adult$relationship)
levels(adult.test$race)<-levels(adult$race)
levels(adult.test$native.country)<-levels(adult$native.country)
importance(rf.adult)
plot(rf.adult)
rf.pred.prob = predict(rf.adult,newdata=testdata,type = "prob")
rf.prob = rf.pred.prob[,2]
roc.rf = performance(results.rf, measure = "tpr", x.measure = "fpr")
plot(roc.rf,colorize = TRUE)
abline(a=0, b= 1)
auc.rf <- performance(results.rf, measure = "auc")
auc.rf <- auc.rf@y.values
text(x = .40, y = .6,paste("AUC = ", round(auc.rf[[1]],3), sep = ""))
# Fix variable levels in adult.test in order to use randomForest
levels(adult.test$workclass)<-levels(adult$workclass)
# Run randomForest prediction
testdata = adult.test[,-15]
# Generate ROC curve to find a cutoff to use
results.rf <-prediction(rf.prob, adult.test$income,label.ordering=c("<=50K.",">50K."))
# Run randomForest prediction
testdata = adult.test[,-15]
rf.pred.prob = predict(rf.adult,newdata=testdata,type = "prob")
rf.prob = rf.pred.prob[,2]
# Generate ROC curve to find a cutoff to use
results.rf <-prediction(rf.prob, adult.test$income,label.ordering=c("<=50K.",">50K."))
roc.rf = performance(results.rf, measure = "tpr", x.measure = "fpr")
plot(roc.rf,colorize = TRUE)
abline(a=0, b= 1)
auc.rf <- performance(results.rf, measure = "auc")
auc.rf <- auc.rf@y.values
text(x = .40, y = .6,paste("AUC = ", round(auc.rf[[1]],3), sep = ""))
##### Find best mtry ####
# Use custom loop to find it based on training model
# Using previous discovered cutoff3 = 0.425
# Adjust this to also find best mtry for AUC
parameter = 10
overall.Accuracy = c()
overall.auc = c()
auc.rf1[[1]]
auc.rf[[1]]
round(auc.rf[[1]],3)
##### Find best mtry ####
# Use custom loop to find it based on training model
# Using previous discovered cutoff3 = 0.425
# Adjust this to also find best mtry for AUC
parameter = 10
overall.Accuracy = c()
overall.auc = c()
for(i in 1:parameter){
set.seed(1)
rf.adult.temp = randomForest(income~.,data=adult,mtry=i)
rf.pred.temp = predict(rf.adult.temp,newdata=testdata,type = "prob")
rf.prob.temp = rf.pred.temp[,2]
class.rf.temp <-factor(ifelse(rf.prob.temp>cutoff3,">50K.","<=50K."),levels=c("<=50K.",">50K."))
cm.temp = confusionMatrix(class.rf.temp,adult.test$income)
overall.Accuracy[i]=cm.temp$overall[1]
# Calculate AUC
results.rf.temp <-prediction(rf.prob.temp, adult.test$income,label.ordering=c("<=50K.",">50K."))
auc.temp <- performance(results.rf.temp, measure = "auc")
auc.temp <- auc.temp@y.values
overall.auc[i] <- round(auc.rf[[1]],3)
}
#Calculate logistic regression complex using cut of 0.425 on test set
cutoff3<-0.425
for(i in 1:parameter){
set.seed(1)
rf.adult.temp = randomForest(income~.,data=adult,mtry=i)
rf.pred.temp = predict(rf.adult.temp,newdata=testdata,type = "prob")
rf.prob.temp = rf.pred.temp[,2]
class.rf.temp <-factor(ifelse(rf.prob.temp>cutoff3,">50K.","<=50K."),levels=c("<=50K.",">50K."))
cm.temp = confusionMatrix(class.rf.temp,adult.test$income)
overall.Accuracy[i]=cm.temp$overall[1]
# Calculate AUC
results.rf.temp <-prediction(rf.prob.temp, adult.test$income,label.ordering=c("<=50K.",">50K."))
auc.temp <- performance(results.rf.temp, measure = "auc")
auc.temp <- auc.temp@y.values
overall.auc[i] <- round(auc.rf[[1]],3)
}
for(i in 1:parameter){
print(paste("Processing #",i))
set.seed(1)
rf.adult.temp = randomForest(income~.,data=adult,mtry=i)
rf.pred.temp = predict(rf.adult.temp,newdata=testdata,type = "prob")
rf.prob.temp = rf.pred.temp[,2]
class.rf.temp <-factor(ifelse(rf.prob.temp>cutoff3,">50K.","<=50K."),levels=c("<=50K.",">50K."))
cm.temp = confusionMatrix(class.rf.temp,adult.test$income)
overall.Accuracy[i]=cm.temp$overall[1]
# Calculate AUC
results.rf.temp <-prediction(rf.prob.temp, adult.test$income,label.ordering=c("<=50K.",">50K."))
auc.temp <- performance(results.rf.temp, measure = "auc")
auc.temp <- auc.temp@y.values
overall.auc[i] <- round(auc.rf[[1]],3)
}
overall.Accuracy
overall.auc
set.seed(1)
rf.adult2 = randomForest(income~.,data=adult,mtry=1)
rf.prob2 = rf.pred.prob2[,2]
roc.rf2 = performance(results.rf2, measure = "tpr", x.measure = "fpr")
plot(roc.rf2,colorize = TRUE)
# Run randomForest prediction
rf.pred.prob2 = predict(rf.adult2,newdata=testdata,type = "prob")
# Generate ROC curve to find a cutoff to use
results.rf2 <-prediction(rf.prob2, adult.test$income,label.ordering=c("<=50K.",">50K."))
set.seed(1)
rf.adult2 = randomForest(income~.,data=adult,mtry=1)
# Run randomForest prediction
rf.pred.prob2 = predict(rf.adult2,newdata=testdata,type = "prob")
rf.prob2 = rf.pred.prob2[,2]
# Generate ROC curve to find a cutoff to use
results.rf2 <-prediction(rf.prob2, adult.test$income,label.ordering=c("<=50K.",">50K."))
roc.rf2 = performance(results.rf2, measure = "tpr", x.measure = "fpr")
plot(roc.rf2,colorize = TRUE)
abline(a=0, b= 1)
auc.rf2 <- performance(results.rf2, measure = "auc")
auc.rf2 <- auc.rf2@y.values
text(x = .40, y = .6,paste("AUC = ", round(auc.rf2[[1]],3), sep = ""))
round(auc.temp[[1]],3)
##### Find best mtry ####
# Use custom loop to find it based on training model
# Using previous discovered cutoff3 = 0.425
# Adjust this to also find best mtry for AUC
parameter = 10
overall.Accuracy = c()
overall.auc = c()
for(i in 1:parameter){
print(paste("Processing mtry=",i))
set.seed(1)
rf.adult.temp = randomForest(income~.,data=adult,mtry=i)
rf.pred.temp = predict(rf.adult.temp,newdata=testdata,type = "prob")
rf.prob.temp = rf.pred.temp[,2]
class.rf.temp <-factor(ifelse(rf.prob.temp>cutoff3,">50K.","<=50K."),levels=c("<=50K.",">50K."))
cm.temp = confusionMatrix(class.rf.temp,adult.test$income)
overall.Accuracy[i]=cm.temp$overall[1]
# Calculate AUC
results.rf.temp <-prediction(rf.prob.temp, adult.test$income,label.ordering=c("<=50K.",">50K."))
auc.temp <- performance(results.rf.temp, measure = "auc")
auc.temp <- auc.temp@y.values
overall.auc[i] <- round(auc.temp[[1]],3)
}
overall.Accuracy
overall.auc
