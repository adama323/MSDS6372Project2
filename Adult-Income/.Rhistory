qt(0.975,224)
1.998664+0.1101327*9.4+0.2001273*2
exp(3.424166)
1.998664+0.1101327*9.4+0.2001273*3
exp(3.634293)
exp(3.627926)
exp(3.640658)
0.010282784-0.030326651
(4+9-8.2)*10^08
sqrt(4.8*10^(-8))
qt(0.975,224)
-0.02004387+1.97*0.000219
-0.02004387-1.97*0.000219
0.714/5
9.381-9.667
9.381-8.667
0.1428/0.619
1-pf(0.231,5,14)
df = c(1,2,3,4,5)
scale(df)
-4.66334+(-.2)*(-26.14277)+(-.2)*18.39440
-1.10386+(-.2)*(-6.82377)+(-.2)*1.27017
-4.66334+(-.2)*(-26.14277)+(0)*18.39440
-1.10386+(-.2)*(-6.82377)+(0)*1.27017
-4.66334+(-.3)*(-26.14277)+(0)*18.39440
-1.10386+(-.3)*(-6.82377)+(0)*1.27017
2.7+2.3*5
2.7+2.3*8
2.7+2.3*25
2.7+2.3*30
((14.2-14)^2+(21.1-22)^2+(60.2-61)^2+(71.7-71)^2)/4
pnorm
?pnorm
pnorm(2.924,0,1)
1-pnorm(2.924,0,1)
2*(1-pnorm(2.924,0,1))
library(MASS)
library(mvtnorm)
library(mvtnorm)
set.seed(1234)
muYes<-c(10,10)
muNo<-c(8,8)
Sigma<-matrix(c(1,.8,.8,1),2,2,byrow=T)
nY<-30
nN<-30
dataYes<-rmvnorm(nY,muYes,Sigma)
dataNo<- rmvnorm(nN,muNo,Sigma)
train<-rbind(dataYes,dataNo)
train<-data.frame(train)
for (i in 3:20){
train<-cbind(train,rnorm(nY+nN))
}
names(train)<-paste("X",1:20,sep="")
train$Response<-rep(c("Yes","No"),each=30)
train$Response<-factor(train$Response)
#Creating a test set
muYes<-c(10,10)
muNo<-c(8,8)
Sigma<-matrix(c(1,.8,.8,1),2,2,byrow=T)
nY<-500
nN<-500
dataYes<-rmvnorm(nY,muYes,Sigma)
dataNo<- rmvnorm(nN,muNo,Sigma)
test<-rbind(dataYes,dataNo)
test<-data.frame(test)
for (i in 3:20){
test<-cbind(test,rnorm(nY+nN))
}
names(test)<-paste("X",1:20,sep="")
test$Response<-rep(c("Yes","No"),each=500)
test$Response<-factor(test$Response)
mylda<-lda(Response~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10,data=train)
pred<-predict(mylda,newdata=test)$class  #Predictions can come in many forms, the class form provides the categorical level of your response.
Truth<-test$Response
x<-table(pred,Truth) # Creating a confusion matrix
x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/1000
ME
#Calculating overall accuracy
1-ME
mylda<-lda(Response~.,data=train)
mylda
mylda<-lda(Response~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10,data=train)
mylda
.5+1.96*sqrt(.5*.5/1000)
.5+1.96*sqrt(.5*.5/1000)
0.516129-0.311475
19*30/42*32
19*30/42/32
32*42/19/30
sqrt(0.516129*(1-0.516129)/62+0.311475*(1-0.311475)/61)
0.08685489*1.96
0.204654-0.1702356
0.204654+0.1702356
log(2.36)
sqrt(1/32+1/30+1/19+1/42)
0.86-1.96*0.3755
0.86+1.96*0.3755
exp(0.124)
exp(1.596)
?pnorm
(32+19)/(62+61)
sqrt(0.4146*(1-0.4146)/61+0.4146*(1-0.4146)/62)
0.204654/0.08884496
pnorm(2.3035,0,1)
1-pnorm(2.3035,0,1)
(1-pnorm(2.3035,0,1))*2
0.86/0.3755
(1-pnorm(2.29,0,1))*2
(76/411)/(105/407)
76*302/105/335
(302/407)/(335/411)
(335/411)/(302/407)
(335/76)/(302/105)
2499.96+1325.56+184.61
4010.13/9208.65
0.56688599*8345.27
library(ggplot2)
library(cowplot)
install.packages("cowplot")
library(cowplot)
url <- "http://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
data <- read.csv(url, header=FALSE)
#####################################
##
## Reformat the data so that it is
## 1) Easy to use (add nice column names)
## 2) Interpreted correctly by glm()..
##
#####################################
head(data) # you see data, but no column names
colnames(data) <- c(
"age",
"sex",# 0 = female, 1 = male
"cp", # chest pain
# 1 = typical angina,
# 2 = atypical angina,
# 3 = non-anginal pain,
# 4 = asymptomatic
"trestbps", # resting blood pressure (in mm Hg)
"chol", # serum cholestoral in mg/dl
"fbs",  # fasting blood sugar if less than 120 mg/dl, 1 = TRUE, 0 = FALSE
"restecg", # resting electrocardiographic results
# 1 = normal
# 2 = having ST-T wave abnormality
# 3 = showing probable or definite left ventricular hypertrophy
"thalach", # maximum heart rate achieved
"exang",   # exercise induced angina, 1 = yes, 0 = no
"oldpeak", # ST depression induced by exercise relative to rest
"slope", # the slope of the peak exercise ST segment
# 1 = upsloping
# 2 = flat
# 3 = downsloping
"ca", # number of major vessels (0-3) colored by fluoroscopy
"thal", # this is short of thalium heart scan
# 3 = normal (no cold spots)
# 6 = fixed defect (cold spots during rest and exercise)
# 7 = reversible defect (when cold spots only appear during exercise)
"hd" # (the predicted attribute) - diagnosis of heart disease
# 0 if less than or equal to 50% diameter narrowing
# 1 if greater than 50% diameter narrowing
)
head(data) # now we have data and column names
str(data) # this shows that we need to tell R which columns contain factors
## First, convert "?"s to NAs...
data[data == "?"] <- NA
## Now add factors for variables that are factors and clean up the factors
## that had missing data...
data[data$sex == 0,]$sex <- "F"
data[data$sex == 1,]$sex <- "M"
data$sex <- as.factor(data$sex)
data$cp <- as.factor(data$cp)
data$fbs <- as.factor(data$fbs)
data$restecg <- as.factor(data$restecg)
data$exang <- as.factor(data$exang)
data$slope <- as.factor(data$slope)
data$ca <- as.integer(data$ca) # since this column had "?"s in it
# R thinks that the levels for the factor are strings, but
# we know they are integers, so first convert the strings to integers...
data$ca <- as.factor(data$ca)  # ...then convert the integers to factor levels
data$thal <- as.integer(data$thal) # "thal" also had "?"s in it.
data$thal <- as.factor(data$thal)
## This next line replaces 0 and 1 with "Healthy" and "Unhealthy"
data$hd <- ifelse(test=data$hd == 0, yes="Healthy", no="Unhealthy")
data$hd <- as.factor(data$hd) # Now convert to a factor
str(data) ## this shows that the correct columns are factors
## Now determine how many rows have "NA" (aka "Missing data"). If it's just
## a few, we can remove them from the dataset, otherwise we should consider
## imputing the values with a Random Forest or some other imputation method.
nrow(data[is.na(data$ca) | is.na(data$thal),])
data[is.na(data$ca) | is.na(data$thal),]
## so 6 of the 303 rows of data have missing values. This isn't a large
## percentage (2%), so we can just remove them from the dataset
## NOTE: This is different from when we did machine learning with
## Random Forests. When we did that, we imputed values.
nrow(data)
data <- data[!(is.na(data$ca) | is.na(data$thal)),]
nrow(data)
##
## Now we can do some quality control by making sure all of the factor
## levels are represented by people with and without heart disease (hd)
##
## NOTE: We also want to exclude variables that only have 1 or 2 samples in
## a category since +/- one or two samples can have a large effect on the
## odds/log(odds)
##
##
#####################################
xtabs(~ hd + sex, data=data)
xtabs(~ hd + cp, data=data)
xtabs(~ hd + fbs, data=data)
xtabs(~ hd + restecg, data=data)
xtabs(~ hd + exang, data=data)
xtabs(~ hd + slope, data=data)
xtabs(~ hd + ca, data=data)
xtabs(~ hd + thal, data=data)
logistic <- glm(hd ~ sex, data=data, family="binomial")
summary(logistic)
logistic <- glm(hd ~ ., data=data, family="binomial")
summary(logistic)
## Now calculate the overall "Pseudo R-squared" and its p-value
ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null)
(ll.null - ll.proposed) / ll.null
## The p-value for the R^2
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))
## Let's start by going through the first coefficient...
## (Intercept)  -1.0438     0.2326  -4.488 7.18e-06 ***
##
## The intercept is the log(odds) a female will be unhealthy. This is because
## female is the first factor in "sex" (the factors are ordered,
## alphabetically by default,"female", "male")
female.log.odds <- log(25 / 71)
female.log.odds
## Now let's look at the second coefficient...
##   sexM        1.2737     0.2725   4.674 2.95e-06 ***
##
## sexM is the log(odds ratio) that tells us that if a sample has sex=M, the
## odds of being unhealthy are, on a log scale, 1.27 times greater than if
## a sample has sex=F.
male.log.odds.ratio <- log((112 / 89) / (25/71))
male.log.odds.ratio
## Lastly, let's  see what this logistic regression predicts, given
## that a patient is either female or male (and no other data about them).
predicted.data <- data.frame(
probability.of.hd=logistic$fitted.values,
sex=data$sex)
## We can plot the data...
ggplot(data=predicted.data, aes(x=sex, y=probability.of.hd)) +
geom_point(aes(color=sex), size=5) +
xlab("Sex") +
ylab("Predicted probability of getting heart disease")
## Since there are only two probabilities (one for females and one for males),
## we can use a table to summarize the predicted probabilities.
xtabs(~ probability.of.hd + sex, data=predicted.data)
logistic <- glm(hd ~ ., data=data, family="binomial")
summary(logistic)
## Now calculate the overall "Pseudo R-squared" and its p-value
ll.null <- logistic$null.deviance/-2
ll.proposed <- logistic$deviance/-2
## McFadden's Pseudo R^2 = [ LL(Null) - LL(Proposed) ] / LL(Null)
(ll.null - ll.proposed) / ll.null
## The p-value for the R^2
1 - pchisq(2*(ll.proposed - ll.null), df=(length(logistic$coefficients)-1))
## now we can plot the data
predicted.data <- data.frame(
probability.of.hd=logistic$fitted.values,
hd=data$hd)
predicted.data <- predicted.data[
order(predicted.data$probability.of.hd, decreasing=FALSE),]
predicted.data$rank <- 1:nrow(predicted.data)
## Lastly, we can plot the predicted probabilities for each sample having
## heart disease and color by whether or not they actually had heart disease
ggplot(data=predicted.data, aes(x=rank, y=probability.of.hd)) +
geom_point(aes(color=hd), alpha=1, shape=4, stroke=2) +
xlab("Index") +
ylab("Predicted probability of getting heart disease")
ggsave("heart_disease_probabilities.pdf")
exp(-1.7361+6.2954*0.5)/(1+exp(-1.7361+6.2954*0.5))
exp(0.0956*(55-45))
exp(-2.9278+0.0956*50)/(1+exp(-2.9278+0.0956*50))
exp(0.315)
1/0.315
exp(-0.2)
#DATA entry and EDA.  You guys are familiar with this data set already.  Its the same as the PCA Unit 9 code.
library(ISLR)
dim(Auto)
newAuto<-Auto
#creating binary response for illustration
newAuto$mpg<-factor(ifelse(Auto$mpg>median(Auto$mpg),"High","Low"))
newAuto$cylinders<-factor(newAuto$cylinders)
newAuto$origin<-factor(newAuto$origin)
newAuto$mpg<-relevel(newAuto$mpg, ref = "Low")  #""Yes/1" is high mpg "No/0" is low mpg.  This forces the odds to be what you want it to be.
#
#aggregate is good for summary stats by groups for continous predictors
aggregate(weight~mpg,data=newAuto,summary)
aggregate(displacement~mpg,data=newAuto,summary)
#lets attach newAuto so we don't have to keep writing newAuto$
attach(newAuto)
#Table of counts like proc freq are helpful for categorcal predictors
ftable(addmargins(table(mpg,cylinders)))
#It probably is wise to throw out the 3 and 5 cylinder ones or combine it with
#four or six.  I'll remove to keep it short.
newAuto<-newAuto[-which(cylinders %in% c(3,5)),]
attach(newAuto)
cylinders=factor(cylinders)
levels(cylinders)
ftable(addmargins(table(mpg,origin)))
ftable(addmargins(table(mpg,year)))
#to get proportions that make sense
prop.table(table(mpg,cylinders),2)
prop.table(table(mpg,origin),2)
prop.table(table(mpg,year),2)
#Visualize
plot(mpg~cylinders,col=c("red","blue"))
plot(mpg~origin,col=c("red","blue"))
plot(mpg~year,col=c("red","blue"))
#Visualize
plot(weight~mpg,col=c("red","blue"))
plot(acceleration~mpg,col=c("red","blue"))
plot(displacement~mpg,col=c("red","blue"))
#Examine the correlation between the continous predictors
pairs(newAuto[,3:6])
my.cor<-cor(newAuto[,3:6])
my.cor
pairs(newAuto[,3:6],col=mpg)
#If you have a lot of predictors, heatmap with correlations could
#be helpful to examine redundancy.
library(gplots)
library(ggplot2)
heatmap.2(my.cor,col=redgreen(75),
density.info="none", trace="none", dendrogram=c("row"),
symm=F,symkey=T,symbreaks=T, scale="none")
#Another option here would be to do PCA among the continous predictors to see
#if they seperate out.  Or a heatmap.
pc.result<-prcomp(newAuto[,3:6],scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$mpg<-newAuto$mpg
#Use ggplot2 to plot the first few pc's
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
geom_point(aes(col=mpg), size=1)+
ggtitle("PCA of Auto")
setwd("/Users/mingyang/Desktop/SMU/Applied Statistics/MSDS6372Project2/Adult-Income")
library(tidyverse)
adult = read.csv("adult.data.csv",header=FALSE,sep=",",strip.white=TRUE)
# Assign column names
colnames(adult) <- c(
"age", # continuous
"workclass",# Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked
"fnlwgt", # continous, final weight. In other words, this is the number of people the census believes the entry represents
"education", # Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th,
# Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.education-num: continuous
"education.num", # continuous
"marital.status", # Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
"occupation",  # Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct,                 # Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces
"relationship", # Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried
"race", # White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black
"sex",   # Female, Male
"capital.gain", # continuous
"capital.loss", # continuous
"hours.per.week", # continuous
"native.country", # United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands
"income" # expects to predict
)
str(adult)
## First, convert "?"s to Not-Known...
adult[adult == "?"] <- "Not-Known"
# Decision is made to delete Not-Known observations under workclass & occupation (based on EDA below)
# 1. workclass and occupation missing values are almost identical - except a few workclass assigned Never-worked has a few Not-Known
# workclass and occupation seem to both being good potential predictors for response income
# Thus decision is made to delete Not-Known under occupation and workclass - restrict our range to only predict observation that has occupation or workclass that is known to us
adult = adult%>% filter(workclass!="Not-Known" & occupation!="Not-Known")
# Reorder education levels for later
edu.levels = factor(c("Preschool","1st-4th","5th-6th","7th-8th","9th","10th","11th","12th","HS-grad","Some-college","Assoc-voc","Assoc-acdm","Bachelors","Masters","Prof-school","Doctorate"))
adult$education <- factor(adult$education, levels = edu.levels)
variables.to.factor = c("workclass","marital.status","occupation","relationship","race","sex","native.country","income")
adult[variables.to.factor] = lapply(adult[variables.to.factor],factor)
summary(adult)
# inporting testing data
adult.test = read.csv("adult.test.csv",header=FALSE,sep=",",strip.white=TRUE)
colnames(adult.test) <- c(
"age", # continuous
"workclass",# Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked
"fnlwgt", # continous, final weight. In other words, this is the number of people the census believes the entry represents
"education", # Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th,
# Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.education-num: continuous
"education.num", # continuous
"marital.status", # Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.
"occupation",  # Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct,                 # Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces
"relationship", # Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried
"race", # White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black
"sex",   # Female, Male
"capital.gain", # continuous
"capital.loss", # continuous
"hours.per.week", # continuous
"native.country", # United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands
"income" # expects to predict
)
## First, convert "?"s to Not-Known...
adult.test[adult.test == "?"] <- "Not-Known"
# Similar decision is made to delete workclass that has Not-Known to utilize both workclass and occupation
adult.test = adult.test %>% filter(workclass!="Not-Known"& occupation!="Not-Known")
adult.test$education <- factor(adult.test$education, levels=edu.levels)
variables.to.factor = c("workclass","marital.status","occupation","relationship","race","sex","native.country","income")
adult.test[variables.to.factor] = lapply(adult.test[variables.to.factor],factor)
summary(adult.test)
#adult.missing = adult %>% filter_all(any_vars(is.na(.)))
#adult.test.missing = adult.test %>% filter_all(any_vars(is.na(.)))
# Look at missing data
library(naniar)
gg_miss_var(adult)
gg_miss_var(adult.test)
library(GGally)
#create temp variable in order to fit lm model to test vif
temp = as.numeric(adult$income)
linear.model = lm(temp~.-income-education.num-occupation,data=adult)
library(car) # where vif lives
summary(linear.model)
vif(linear.model)
# remove relationship variable see if GVIF gets better
linear.model1 = lm(temp~.-income-education.num-occupation-relationship,data=adult)
vif(linear.model1)
table(marital.status,relationship)
# Which variable is more associated with the response though? - try chi-squared test approximation(since dataset is quite large)
attach(adult)
chi.test <- chisq.test(table(income,marital.status))
chi.test
chi.test1 <- chisq.test(table(income,relationship))
chi.test1
# About the same levels of association
summary(marital.status)
summary(relationship)
# Note education.num and education may also be strongly correlated
plot(education.num~education)
# Try getting rid of relationship and education.num and see if occupation is still an issue with GVIF
linear.model2 = lm(temp~.-income-education.num-relationship,data=adult)
summary(linear.model2)
vif(linear.model2)
# summary(linear.model2) tells us there might be a perfect correlation between Not-Known between workclass and occupation
# Following code to verify if this is true
adult.temp = adult %>% filter(workclass=="Not-Known"|occupation=="Not-Known") %>% select(workclass,occupation)
#view(adult.temp)
# There is a perfect correlation existed between missingi values of workclass and occupation in original dataset
summary(workclass)
summary(occupation)
# Take a look at whether workclass and occupation are independent from each other
chi.test2 <- chisq.test(table(occupation,workclass))
chi.test2
# Check whether occupation contribute to predicting response variable
chi.test3 <- chisq.test(table(income,occupation))
chi.test3
# Check whether workclass contribute to predicting response variable
chi.test4 <- chisq.test(table(income,workclass))
chi.test4
ftable(addmargins(table(income,sex))) # sex can be contributing to income
ftable(addmargins(table(income,workclass)))
ftable(addmargins(table(income,education))) # Master, Prof-school, Doctorate all have more people making over 50k
ftable(addmargins(table(income,marital.status)))
ftable(addmargins(table(income,occupation)))
ftable(addmargins(table(income,race)))
ftable(addmargins(table(income,native.country)))
(addmargins(table(income,native.country)))
addmargins(table(income,native.country))
addmargins(table(income,native.country))
?ftable
ftable(addmargins(table(income,occupation)))
addmargins(table(income,occupation))
plot(income~sex,col=c("red","blue"))
plot(income~workclass,col=c("red","blue"))
plot(income~education,col=c("red","blue"))
plot(income~marital.status,col=c("red","blue"))
plot(income~relationship,col=c("red","blue"))
plot(income~occupation,col=c("red","blue"))
plot(income~race,col=c("red","blue"))
plot(income~native.country,col=c("red","blue"))
#DATA entry and EDA.  You guys are familiar with this data set already.  Its the same as the PCA Unit 9 code.
library(ISLR)
dim(Auto)
newAuto<-Auto
#creating binary response for illustration
newAuto$mpg<-factor(ifelse(Auto$mpg>median(Auto$mpg),"High","Low"))
newAuto$cylinders<-factor(newAuto$cylinders)
newAuto$origin<-factor(newAuto$origin)
newAuto$mpg<-relevel(newAuto$mpg, ref = "Low")  #""Yes/1" is high mpg "No/0" is low mpg.  This forces the odds to be what you want it to be.
#
#aggregate is good for summary stats by groups for continous predictors
aggregate(weight~mpg,data=newAuto,summary)
aggregate(displacement~mpg,data=newAuto,summary)
#lets attach newAuto so we don't have to keep writing newAuto$
attach(newAuto)
#Table of counts like proc freq are helpful for categorcal predictors
ftable(addmargins(table(mpg,cylinders)))
#It probably is wise to throw out the 3 and 5 cylinder ones or combine it with
#four or six.  I'll remove to keep it short.
newAuto<-newAuto[-which(cylinders %in% c(3,5)),]
attach(newAuto)
cylinders=factor(cylinders)
levels(cylinders)
ftable(addmargins(table(mpg,origin)))
ftable(addmargins(table(mpg,year)))
#to get proportions that make sense
prop.table(table(mpg,cylinders),2)
prop.table(table(mpg,origin),2)
prop.table(table(mpg,year),2)
#Visualize
plot(mpg~cylinders,col=c("red","blue"))
plot(mpg~origin,col=c("red","blue"))
#Visualize
plot(mpg~cylinders,col=c("red","blue"))
